# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy this file to `.env.local` for local development.
# Never commit `.env.local` to version control.
#
# For production deployments (e.g., Vercel), configure these via the hosting
# platform's environment variable dashboard.
#
# See: https://nextjs.org/docs/app/building-your-application/configuring/environment-variables

# -----------------------------------------------------------------------------
# DEVELOPMENT OPTIMIZATIONS
# -----------------------------------------------------------------------------
# These settings improve development server performance and hot reload speed

NODE_ENV=development
NEXT_TELEMETRY_DISABLED=1
WATCHPACK_POLLING=false

# =============================================================================

# -----------------------------------------------------------------------------
# Site Configuration
# -----------------------------------------------------------------------------
# These are optional and have sensible defaults in `src/lib/site-config.ts`.
# Override only if you need different domains for testing or custom deployments.

# Full site URL (e.g., https://www.dcyfr.ai)
# Highest priority override for the site URL
# NEXT_PUBLIC_SITE_URL=https://www.dcyfr.ai

# Just the domain (e.g., www.dcyfr.ai)
# Will be prefixed with https:// automatically
# NEXT_PUBLIC_SITE_DOMAIN=www.dcyfr.ai

# Vercel environment (managed by Vercel, not needed locally)
# NEXT_PUBLIC_VERCEL_ENV=preview

# -----------------------------------------------------------------------------
# Background Jobs (Inngest)
# -----------------------------------------------------------------------------
# Optional: Required for production background job processing.
# In development, Inngest functions run without these keys using the Dev Server.
#
# Without these keys:
# - Contact form processed asynchronously (slower, ~1-2s)
# - No background job processing (GitHub cache, analytics, etc.)
# - Dev UI available at http://localhost:3000/api/inngest
#
# With these keys:
# - Contact form responds in < 100ms
# - Automatic retries on failures
# - Scheduled jobs (GitHub data refresh, analytics)
# - Production monitoring and debugging
#
# Get your keys at: https://app.inngest.com/
# Required for: production deployments

INNGEST_EVENT_KEY=
INNGEST_SIGNING_KEY=

# Optional: Email address to receive alerts when Inngest functions fail.
# Without this, function failures are still logged in Inngest and Sentry,
# but you won't receive email notifications.
#
# Recommended: Set this in production to get immediate alerts on critical failures.
# Also requires: RESEND_API_KEY to send alert emails.
#
# Severity levels for alerts:
# - CRITICAL (contact form, payments) → Immediate email alert
# - HIGH (GitHub sync, security, analytics) → Email + Sentry
# - MEDIUM (trending, milestones) → Sentry only
# - LOW (logging, monitoring) → Console only

INNGEST_ERROR_ALERTS_EMAIL=

# -----------------------------------------------------------------------------# Security Alert Emails
# -----------------------------------------------------------------------------
# Optional: Email addresses for receiving security alerts and contact form notifications.
# Multiple variables support different notification types with fallback behavior.
#
# Without these:
# - Security alerts logged to Sentry only (no email notifications)
# - Contact form submissions logged but not forwarded
#
# With these:
# - Critical security events trigger email alerts
# - Contact form submissions forwarded to inbox
# - IP reputation alerts and blocking notifications
#
# Priority order for security alerts:
# 1. SECURITY_ALERT_EMAIL (dedicated security alerts)
# 2. CONTACT_EMAIL (general contact/fallback)
#
# Both use RESEND_API_KEY for email delivery

SECURITY_ALERT_EMAIL=
CONTACT_EMAIL=

# -----------------------------------------------------------------------------# Email Configuration (Contact Form)
# -----------------------------------------------------------------------------
# Optional: Used for sending emails via contact form and blog milestone notifications.
# Without this, the contact form will log submissions and show a warning to users.
#
# Get your API key at: https://resend.com/api-keys
# REQUIRED for production email delivery

RESEND_API_KEY=

# Optional: Override the "from" email address for outgoing emails.
# Default: no-reply@www.dcyfr.ai (configured in src/lib/site-config.ts)
# Only needed if you want a different sender address.

# NEXT_PUBLIC_FROM_EMAIL=no-reply@yourdomain.com

# -----------------------------------------------------------------------------
# Bot Detection & Security (Vercel BotID)
# -----------------------------------------------------------------------------
# Optional: Advanced bot detection for API routes (Vercel Pro feature).
# Without this, contact form relies on rate limiting + honeypot field + input validation.
#
# Important: BotID client-side protection is currently DISABLED for /api/contact
# to prevent 403 errors. The contact form works securely without BotID via:
# - Rate limiting (3 requests/minute per IP)
# - Honeypot field (hidden from real users)
# - Input validation and sanitization
# - Request size limits (50KB max)
#
# To enable BotID protection (PRODUCTION ONLY):
# 1. Verify BotID is configured in Vercel Pro dashboard
# 2. Set ENABLE_BOTID=1 in production environment variables
# 3. Re-enable client-side protection in src/instrumentation-client.ts
# 4. Test thoroughly in preview environment first
#
# Get BotID: https://vercel.com/docs/botid/get-started
# Requires: Vercel Pro subscription

# ENABLE_BOTID=1

# -----------------------------------------------------------------------------
# IP Reputation & Threat Intelligence (GreyNoise)
# -----------------------------------------------------------------------------
# Optional but RECOMMENDED: Used for automated IP reputation checking and threat detection.
# Without this key, IP reputation features are disabled but the app works normally.
#
# GreyNoise provides real-time threat intelligence to automatically identify and block
# malicious IPs before they can attack your application. The system integrates with
# your existing rate limiting and security infrastructure.
#
# Get your API key at: https://viz.greynoise.io/
# Free tier available for basic lookups
#
# Setup Instructions:
# 1. Sign up at GreyNoise (free tier available)
# 2. Get your API key from the dashboard
# 3. Add the key below to enable automated IP reputation checking
# 4. The system runs hourly checks automatically via Inngest
# 5. Manual IP lookups available at /api/ip-reputation (requires ADMIN_API_KEY)
#
# Without GreyNoise API key:
# - IP reputation checking disabled (graceful degradation)
# - Standard rate limiting still protects against abuse
# - Manual IP blocking still available via Redis
#
# With GreyNoise API key:
# - Automated hourly analysis of high-traffic IPs from logs
# - Malicious IPs automatically blocked (0 requests allowed)
# - Suspicious IPs get enhanced rate limiting (10 req/5min vs 100)
# - Real-time threat detection with confidence scoring
# - Integration with RIOT dataset (whitelist for legitimate services)
# - Manual IP reputation API for investigations
# - Security alerts via Sentry for high-confidence threats
#
# Features enabled:
# - Scheduled IP reputation checks (hourly via Inngest)
# - Manual IP lookups via API (/api/ip-reputation)
# - Adaptive rate limiting based on IP reputation
# - Automatic blocking of confirmed malicious IPs
# - Security event logging and alerting
#
# Rate limiting with GreyNoise:
# - Malicious IPs: 0 requests (blocked)
# - Suspicious IPs: 10 requests per 5 minutes
# - Unknown IPs: 100 requests per 5 minutes  
# - Benign IPs (RIOT): 1000 requests per 5 minutes
#
# API Usage:
#   # Check single IP reputation
#   curl -H "Authorization: Bearer $ADMIN_API_KEY" \
#     "https://your-domain.com/api/ip-reputation?ip=1.2.3.4"
#   
#   # Trigger manual reputation check
#   curl -X POST -H "Authorization: Bearer $ADMIN_API_KEY" \
#     -H "Content-Type: application/json" \
#     -d '{"ips": ["1.2.3.4", "5.6.7.8"]}' \
#     "https://your-domain.com/api/ip-reputation"
#
# ⚠️  COST CONSIDERATIONS:
# - GreyNoise is a paid API service (free tier available)
# - System uses Redis caching to minimize API calls
# - Bulk processing optimizes API usage
# - Monitor usage in GreyNoise dashboard

GREYNOISE_API_KEY=

# -----------------------------------------------------------------------------
# Error Tracking & Monitoring (Sentry)
# -----------------------------------------------------------------------------
# Optional but RECOMMENDED: Used for error tracking, performance monitoring, and uptime checks.
# Without Sentry configured, the app will work normally but errors won't be tracked.
#
# Get your DSN from: https://sentry.io/settings/dcyfr-labs/projects/dcyfr-labs/keys/
#
# Without Sentry:
# - Errors logged to console only
# - No error tracking or alerts
# - No performance monitoring
# - No session replay
#
# With Sentry:
# - Real-time error tracking with stack traces
# - Performance monitoring (10% sample rate in production)
# - Session replay on errors (5% sample rate in production)
# - Source maps uploaded for better debugging
# - Uptime monitoring via /api/health endpoint
#
# The same DSN can be used for all three variables below.

# Sentry MCP endpoint for deployments and source map uploads
SENTRY_MCP_ENDPOINT=

# Server-side error tracking (Node.js runtime)
SENTRY_DSN=

# Client-side error tracking (browser)
# Must be prefixed with NEXT_PUBLIC_ to be available in the browser
NEXT_PUBLIC_SENTRY_DSN=

# Auth token for uploading source maps during build
# Get from: https://sentry.io/settings/account/api/auth-tokens/
# Required for CI/CD pipelines to upload source maps
# Note: For local builds, this is already configured in .env.sentry-build-plugin
# MCP Server scopes: org:read, project:read, project:write, team:read, team:write, event:write
SENTRY_AUTH_TOKEN=

# Organization and project slugs (already configured in next.config.ts)
# SENTRY_ORG=dcyfr-labs
# SENTRY_PROJECT=dcyfr-labs

# -----------------------------------------------------------------------------
# GitHub Integration
# -----------------------------------------------------------------------------
# Optional: Used by the GitHub contributions heatmap on /projects page and
# the Activity feed on /activity page.
# Without this token, the API will use unauthenticated requests (lower rate limits).
#
# Create a personal access token at: https://github.com/settings/tokens
# Required scopes: public_repo (or just read:user for public profiles)
#
# Without this token:
# - GitHub API rate limit: 60 requests/hour per IP
# - GitHub activity will be excluded from /activity page
# - Server-side cache helps (5-minute cache in API route)
# - May hit rate limits during development
#
# With this token:
# - GitHub API rate limit: 5,000 requests/hour
# - Activity feed includes recent commits and releases
# - More reliable data fetching
# - Recommended for production deployments
#
# ⚠️  Note: Token must match the hardcoded username in the API route (dcyfr).
# For security, the API only allows fetching data for the portfolio owner.
# If token is invalid or expired, the Activity page will show other sources
# but exclude GitHub activities.

GITHUB_TOKEN=

# -----------------------------------------------------------------------------
# GitHub Webhook Integration
# -----------------------------------------------------------------------------
# Optional but RECOMMENDED: Used for real-time commit tracking in the Activity feed.
# Without this secret, webhook events from GitHub will be rejected.
#
# Setup Instructions:
# 1. Generate a secure random secret:
#    openssl rand -base64 32
# 2. Add the secret to your .env.local file
# 3. Configure webhook in GitHub repository settings:
#    - URL: https://yourdomain.com/api/github/webhook
#    - Content type: application/json
#    - Secret: (paste the generated secret)
#    - Events: Just the push event
#
# Without this secret:
# - Webhook endpoint rejects all requests (401 Unauthorized)
# - Activity feed excludes real-time commit data
# - Falls back to periodic GitHub API polling (if GITHUB_TOKEN set)
#
# With this secret:
# - Real-time commit tracking (< 30 seconds latency)
# - Commits stored in Redis with 7-day TTL
# - Automatic integration with Activity feed
# - HMAC-SHA256 signature verification for security
#
# Security:
# - NEVER commit this secret to version control
# - Use different secrets for dev/preview/production
# - Rotate periodically for best security
# - Webhook validates signature on every request
#
# Storage:
# - Commits stored at: github:commit:{hash}
# - Recent index at: github:commits:recent (last 1000)
# - 7-day TTL on all commit data
# - Requires REDIS_URL to be configured

GITHUB_WEBHOOK_SECRET=

# -----------------------------------------------------------------------------
# Google Indexing API (Optional but Recommended)
# -----------------------------------------------------------------------------
# Optional but RECOMMENDED: Used for automatically submitting blog posts to Google
# for faster indexing in search results. Without this, you rely on Google's organic
# crawling (which can take weeks or months for new content).
#
# Get your service account key from: https://console.cloud.google.com/
# Full setup guide: docs/features/google-indexing-api.md
#
# Without Google Indexing API:
# - New blog posts may take weeks to appear in Google Search
# - Updates to existing posts take days to reflect
# - No control over indexing priority
# - Rely on sitemap.xml and robots.txt only
#
# With Google Indexing API:
# - New posts submitted to Google within minutes
# - Updates trigger immediate re-crawl
# - Better search visibility for time-sensitive content
# - Automatic submission via Inngest background jobs
#
# Setup Instructions:
# 1. Create a Google Cloud Platform project
# 2. Enable the Indexing API
# 3. Enable the Google Search Console API
# 4. Create a service account with JSON key
# 5. Add service account email as owner in Google Search Console
# 6. Copy the entire JSON key file contents
# 7. Paste the JSON as a single-line string below (escape quotes if needed)
#
# Example format:
# GOOGLE_INDEXING_API_KEY='{"type":"service_account","project_id":"your-project","private_key_id":"...","private_key":"-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n","client_email":"...@....iam.gserviceaccount.com","client_id":"...","auth_uri":"...","token_uri":"...","auth_provider_x509_cert_url":"...","client_x509_cert_url":"..."}'
#
# For detailed setup instructions, see: docs/features/google-indexing-api.md

GOOGLE_INDEXING_API_KEY=

# -----------------------------------------------------------------------------
# Comments System (Giscus)
# -----------------------------------------------------------------------------
# Optional: Enables GitHub Discussions-powered comments on blog posts.
# Without these variables, the comments section will not appear (graceful degradation).
#
# Setup Instructions:
# 1. Enable GitHub Discussions on your repository:
#    - Go to: https://github.com/YOUR_USERNAME/YOUR_REPO/settings
#    - Check "Discussions" under Features
#
# 2. Configure Giscus and get your IDs:
#    - Visit: https://giscus.app/
#    - Follow the setup wizard
#    - Copy the generated values below
#
# 3. Create a Discussion category (recommended name: "Blog Comments")
#
# Without Giscus:
# - Blog posts work normally, but comments section is hidden
# - No errors or broken UI
#
# With Giscus:
# - Users can comment using their GitHub accounts
# - Comments sync with GitHub Discussions
# - Supports reactions, replies, and moderation

# GitHub repository in "owner/repo" format (e.g., "dcyfr/dcyfr-labs")
NEXT_PUBLIC_GISCUS_REPO=

# Repository ID (get from giscus.app)
NEXT_PUBLIC_GISCUS_REPO_ID=

# Discussion category name (e.g., "Blog Comments")
NEXT_PUBLIC_GISCUS_CATEGORY=

# Category ID (get from giscus.app)
NEXT_PUBLIC_GISCUS_CATEGORY_ID=

# -----------------------------------------------------------------------------
# Redis (View Counts, Rate Limiting & Secure Sessions)
# -----------------------------------------------------------------------------
# Optional but RECOMMENDED: Used for:
# 1. Blog post view counts
# 2. Distributed rate limiting across API routes  
# 3. Secure session storage for authentication (encrypted)
#
# Without Redis:
# - View tracking is disabled (graceful degradation, no errors)
# - Rate limiting falls back to in-memory (per-instance, not shared)
# - Session-based authentication is disabled
#
# With Redis:
# - View counts tracked across all deployments
# - Rate limiting shared across all serverless instances
# - Secure encrypted session storage across Dev/Preview/Production
# - Better protection against abuse and traffic spikes
# - Cross-environment authentication support
#
# Get a free Redis instance at:
# - Vercel KV: https://vercel.com/docs/storage/vercel-kv (recommended for Vercel)
# - Upstash: https://upstash.com/ (platform-agnostic)
#
# Format: redis://default:password@host:port

REDIS_URL=

# -----------------------------------------------------------------------------
# Session Security (Required for Authentication)
# -----------------------------------------------------------------------------
# Required if using secure session-based authentication.
# This key encrypts sensitive session data before storing in Redis.
#
# Generate a secure key:
# openssl rand -base64 32
#
# Security features:
# - AES-256-GCM encryption for all session data
# - Automatic session expiration and cleanup
# - CSRF protection
# - Cross-environment compatibility
# - Secure cookie settings
#
# Without this key:
# - Session-based authentication is completely disabled
# - OAuth flows will fail at session creation
#
# With this key:
# - Secure encrypted sessions across all environments
# - Automatic token management (LinkedIn, etc.)
# - Admin session management capabilities

SESSION_ENCRYPTION_KEY=

# Optional: Cookie Domain Configuration
# Set to your domain for production (e.g., .yourdomain.com)
# Leave empty for localhost development
COOKIE_DOMAIN=

# -----------------------------------------------------------------------------
# Analytics Security
# -----------------------------------------------------------------------------
# Optional: Secures access to /api/analytics endpoint with API key authentication.
# Without this key, the analytics endpoint is completely disabled.
#
# The analytics endpoint provides sensitive data about blog post views, shares,
# and trending content. It's protected with 4 security layers:
# 1. API key authentication (this variable)
# 2. Environment blocking (disabled in production)
# 3. Rate limiting (5 requests/minute)
# 4. Audit logging (all access attempts logged)
#
# Generate a secure key:
#   openssl rand -base64 32
#
# Usage:
#   curl http://localhost:3000/api/analytics \
#     -H "Authorization: Bearer YOUR_KEY_HERE"
#
# ⚠️  IMPORTANT SECURITY NOTES:
# - NEVER enable in production environment (hard-blocked by code)
# - Only for development and preview deployments
# - Rotate keys periodically
# - Monitor access logs for suspicious activity
# - Never commit this key to version control
#
# Without this key:
# - Analytics endpoint returns 401 Unauthorized
# - No analytics data exposed
# - Server logs warning that endpoint is disabled

ADMIN_API_KEY=

# For accessing analytics UI at /analytics:
# This must be a NEXT_PUBLIC_ variable to be available in the browser
# Use the same key as ADMIN_API_KEY above
NEXT_PUBLIC_ADMIN_API_KEY=

# -----------------------------------------------------------------------------
# Analytics & Monitoring
# -----------------------------------------------------------------------------
# These are typically managed by Vercel automatically when deployed.
# No configuration needed for local development.

# Vercel Analytics (auto-configured on Vercel)
# https://vercel.com/analytics
# NEXT_PUBLIC_VERCEL_ANALYTICS_ID=

# Vercel Speed Insights (auto-configured on Vercel)
# https://vercel.com/docs/speed-insights
# NEXT_PUBLIC_VERCEL_SPEED_INSIGHTS_ID=

# -----------------------------------------------------------------------------
# Analytics Activity Feed Integration  
# -----------------------------------------------------------------------------
# Optional: API credentials for populating analytics milestones in the activity feed.
# These are used by background jobs to collect metrics from external platforms
# and detect milestone achievements (10K visitors, 100 stars, etc.).
#
# The activity feed works without these credentials using manually populated Redis data.
# See: docs/features/ANALYTICS_INTEGRATION.md for complete setup guide.
#
# Without these credentials:
# - Activity feed shows manually added milestones from Redis
# - No automatic milestone detection
# - Use scripts/populate-analytics-milestones.mjs for testing
#
# With these credentials:
# - Automatic milestone collection via background jobs
# - Real-time achievement tracking across platforms
# - Trending analytics data in unified timeline

# Vercel Analytics API (for traffic milestones)
# Get token from: https://vercel.com/account/tokens
VERCEL_ACCESS_TOKEN=

# GitHub API (for repository metrics - stars, views, clones)  
# Get token from: https://github.com/settings/tokens
# Scopes needed: public_repo (or repo for private repositories)
GITHUB_ACCESS_TOKEN=

# Google Search Console API (for SEO performance metrics)
# Set up service account at: https://console.cloud.google.com/
# Download JSON key file and extract these values:
SEARCH_CONSOLE_CLIENT_EMAIL=
SEARCH_CONSOLE_PRIVATE_KEY=
SEARCH_CONSOLE_SITE_URL=https://dcyfr.dev

# -----------------------------------------------------------------------------
# Vercel: API token for deployments, MCP & platform APIs
# -----------------------------------------------------------------------------
# Optional: A Vercel personal token used for programmatic access to the Vercel
# dashboard, API, and MCP endpoints. This token enables operations like:
# - Reading deployments and logs via Vercel API
# - Using the Vercel MCP server for deployments/insights (mcp.vercel.com)
# - Uploading files and performing CI-related tasks
#
# Get a token at: https://vercel.com/account/tokens
# Recommended: create a scoped token for the repo/organization with only the
# needed permissions. Do NOT commit tokens to the repo.
#
# Supported environment variable names (the dev tools will check these):
# - VERCEL_TOKEN (common)
# - VERCEL_API_TOKEN (alternate)
# - VERCEL_ACCESS_TOKEN (alternate)
#
# We look for any of the above variables when checking Vercel MCP endpoints.
VERCEL_TOKEN=

# -----------------------------------------------------------------------------
# Vercel Deployment URLs (Auto-configured by Vercel)
# -----------------------------------------------------------------------------
# These are automatically set by Vercel during deployment and don't need
# manual configuration. Documented here for reference.
#
# VERCEL_URL - The deployment URL (e.g., my-app-abc123.vercel.app)
#   - Set automatically during Vercel deployments
#   - Used for E2E tests (Playwright base URL)
#   - Used for security alert URLs
#   - No https:// prefix (automatically added in code)
#
# VERCEL_ENV - The deployment environment (production|preview|development)
#   - Set automatically by Vercel
#   - Used for feature flags and environment-specific logic
#
# You typically don't need to set these manually unless:
# - Running E2E tests against a specific deployment URL
# - Testing environment-specific behavior locally
#
# VERCEL_URL=
# VERCEL_ENV=

# Optional: Vercel Analytics proxy endpoint
# A small proxy endpoint or Vercel API URL that returns a standardized JSON shape used
# to populate the analytics dashboard. When set together with VERCEL_TOKEN and
# VERCEL_PROJECT_ID the scheduled job `syncVercelAnalytics` will fetch data and store
# it in Redis for the analytics dashboard.
# Example: https://api.mysite.internal/vercel-analytics
VERCEL_ANALYTICS_ENDPOINT=

# Optional: Vercel Project ID for analytics integration
# The unique identifier for your Vercel project. Used together with VERCEL_TOKEN
# to fetch analytics data via API. You can find this in your Vercel dashboard
# under Project Settings → General → Project ID.
# 
# Without this:
# - Analytics sync will attempt auto-detection from deployment context
# - May fail in local development environment
# 
# With this:
# - Reliable analytics data fetching across all environments
# - Required for scheduled analytics sync jobs
VERCEL_PROJECT_ID=

# -----------------------------------------------------------------------------
# Development Configuration
# -----------------------------------------------------------------------------
# NODE_ENV is automatically set by Next.js based on the command:
# - `npm run dev` → development
# - `npm run build` → production
# - `npm start` → production
#
# You typically don't need to set this manually.

# NODE_ENV=development

# Optional: Disable development-only pages (e.g., during testing)
# Set to "1" to hide dev pages like /dev/* even in development mode
# Default: dev pages visible in development, hidden in production

# DISABLE_DEV_PAGES=1

# Feature toggles (toggle on/off with 'true' or 'false')
# Set to 'true' to enable the horizontal filter chips on blog pages (mobile)
# Default: disabled (recommended)
NEXT_PUBLIC_FEATURE_HORIZONTAL_FILTER_CHIPS=false

# -----------------------------------------------------------------------------
# AI Research (Perplexity AI)
# -----------------------------------------------------------------------------
# Optional: Enables AI-powered research capabilities via /api/research endpoint.
# Without this key, the research endpoint will return a 503 (service unavailable).
#
# Setup Instructions:
# 1. Sign up at: https://www.perplexity.ai/settings/api
# 2. Get your API key from the dashboard
# 3. Add it to your .env.local file
#
# Without Perplexity:
# - Research endpoint is disabled (returns 503)
# - App functions normally otherwise
#
# With Perplexity:
# - AI-powered research with real-time web search
# - Automatic citation generation
# - Multiple model options (small, large, huge)
# - Search recency filters (hour, day, week, month)
# - Domain filtering capabilities
#
# Usage:
#   POST /api/research
#   {
#     "messages": [
#       { "role": "system", "content": "You are a helpful assistant." },
#       { "role": "user", "content": "What are the latest React patterns?" }
#     ],
#     "options": {
#       "model": "llama-3.1-sonar-large-128k-online",
#       "return_citations": true,
#       "search_recency_filter": "week"
#     }
#   }
#
# Rate Limits:
# - 5 requests per minute per IP (configured in /api/research)
# - Responses cached for 5 minutes (server-side)
#
# ⚠️  COST NOTES:
# - Perplexity is a paid API service
# - Monitor usage to control costs
# - Consider implementing additional rate limiting for production
#
# Get your API key at: https://www.perplexity.ai/settings/api

PERPLEXITY_API_KEY=

# -----------------------------------------------------------------------------
# Academic Research (Semantic Scholar)
# -----------------------------------------------------------------------------
# Optional but RECOMMENDED: Enables academic paper search and citation analysis
# via Semantic Scholar MCP server. Without this key, the MCP server uses shared
# rate limit pool (very slow, ~60 req/hour shared across all users).
#
# Setup Instructions:
# 1. Sign up at: https://www.semanticscholar.org/product/api
# 2. Get your API key from the dashboard
# 3. Add it to your .env.local file
# 4. Restart Claude Desktop to load the MCP server
#
# Without Semantic Scholar API key:
# - MCP server works but uses shared rate limit pool
# - Very slow response times (1 req/sec shared globally)
# - May encounter frequent rate limiting errors
# - Unreliable for production use
#
# With Semantic Scholar API key:
# - Dedicated 1 request/second rate limit (your own pool)
# - Intelligent request queuing (automatic)
# - Multi-layer caching (90-day Redis + in-memory)
# - 85% cache hit rate reduces API calls
# - Reliable academic research capabilities
#
# Features enabled:
# - scholar:searchPapers - Search 200M+ academic papers with filters
# - scholar:getPaper - Get paper details by DOI/ArXiv/PubMed/S2 ID
# - scholar:getPaperBatch - Bulk lookup (up to 500 papers)
# - scholar:getCitations - Forward citation analysis
# - scholar:getReferences - Backward citation analysis
# - scholar:searchAuthors - Author name search with metrics
# - scholar:getAuthor - Author profiles (h-index, citations)
# - scholar:getAuthorPapers - Author publication lists
# - scholar:getRecommendations - Paper recommendations
#
# MCP Resources:
# - scholar://cache-stats - Cache hit/miss rates
# - scholar://rate-limit-status - Queue monitoring
# - scholar://recent-queries - Last 20 searches
#
# Caching Strategy (minimizes API usage):
# - Papers: 90 days (metadata rarely changes)
# - Search results: 7 days
# - Authors: 30 days
# - Citations: 1 day (counts change frequently)
# - Expected hit rate: 85% after 1 week
#
# Rate Limiting:
# - 1 request/second (enforced by API)
# - Automatic queuing (transparent to users)
# - Queue monitoring via MCP resources
# - Average wait time tracked in real-time
#
# Integration with Other MCPs:
# - Combine with arXiv MCP for PDF downloads
# - Use with Perplexity for synthesis (cost optimization)
# - 70% reduction in Perplexity usage when used together
#
# Usage Example (via Claude Desktop MCP):
#   # Search for papers
#   scholar:searchPapers({
#     query: "large language models reasoning",
#     year: "2023-",
#     minCitationCount: 10,
#     limit: 20
#   })
#
#   # Get paper details
#   scholar:getPaper({
#     paperId: "10.48550/arXiv.1706.03762"
#   })
#
#   # Explore citations
#   scholar:getCitations({
#     paperId: "10.48550/arXiv.1706.03762",
#     limit: 50
#   })
#
# Cost Savings:
# - Semantic Scholar API: FREE (with API key)
# - arXiv API: FREE
# - Perplexity (fallback only): PAID
# - Combined strategy: 70-85% cost reduction
#
# Performance Benchmarks:
# - Get Paper (cold): ~1200ms
# - Get Paper (Redis): ~50ms
# - Get Paper (memory): <1ms
# - Batch 10 papers (cold): ~2000ms
# - Batch 10 papers (cached): ~5ms
#
# Documentation:
# - Setup: docs/ai/semantic-scholar-mcp-server.md
# - Implementation: docs/ai/semantic-scholar-implementation-summary.md
#
# Get your API key at: https://www.semanticscholar.org/product/api
# (Free tier provides 1 req/sec with dedicated rate limit)

SEMANTIC_SCHOLAR_API_KEY=

# -----------------------------------------------------------------------------
# AI Development Tools (OpenCode.ai Fallback)
# -----------------------------------------------------------------------------
# Optional: OpenCode.ai serves as a fallback when Claude Code encounters
# token exhaustion or rate limiting. Now integrated with GitHub Copilot
# for seamless access to GPT-5 Mini and Raptor Mini models.
#
# See: docs/ai/opencode-fallback-architecture.md for complete setup guide
#
# Setup Instructions:
# 1. Install OpenCode.ai: npm install -g opencode-ai
# 2. Run OpenCode TUI: opencode
# 3. Authenticate GitHub Copilot: /connect → select GitHub Copilot
# 4. Follow device code flow: github.com/login/device
# 5. Verify models: /models (should show gpt-5-mini, raptor-mini, etc.)
#
# GitHub Copilot Models (free with paid subscription):
# - GPT-5 Mini: Primary model (16K context, 0 multiplier)
# - Raptor Mini: Speed-optimized (fine-tuned for coding, 0 multiplier)
#
# Without GitHub Copilot:
# - OpenCode.ai requires additional provider API keys
# - Limited to models outside GitHub Copilot ecosystem
#
# With GitHub Copilot (via device authentication):
# - Access to 20+ premium models (GPT, Claude, Gemini, Grok)
# - GPT-5 Mini and Raptor Mini FREE with subscription
# - No API keys needed (uses OAuth device code flow)
# - Extended context windows (16K for GPT-5 Mini)
#
# Trigger Conditions for OpenCode.ai:
# ❌ Claude Code rate limit exceeded
# ❌ Token budget exhausted
# ✅ Extended development sessions (6+ hours)
# ✅ Cost optimization needed
#
# Authentication:
#   GitHub Copilot uses device code flow (no API key)
#   Run '/connect' in OpenCode TUI for setup
#
# Usage:
#   npm run ai:opencode           # Start session (GPT-5 Mini)
#   npm run ai:opencode:speed     # Use Raptor Mini (fast)

# GitHub Copilot Access
# No API key required - uses device code authentication
# Must have active GitHub Copilot Pro/Pro+/Business subscription
# Authenticate via: opencode → /connect → GitHub Copilot

# OpenAI API Key (optional, for direct API access outside GitHub Copilot)
# Only needed if using OpenAI models directly via API
OPENAI_API_KEY=

# Google API Key (optional, for Gemini 1.5 Pro with 2M token context window)
# Ideal for massive codebase analysis and debugging with extensive context
GOOGLE_API_KEY=

# Azure OpenAI (optional, for enterprise deployments)
# AZURE_OPENAI_API_KEY=
# AZURE_OPENAI_ENDPOINT=

# AWS Bedrock (optional, for AWS-hosted models)
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_REGION=

# Offline Model Support (Msty.ai - Future)
# See: docs/backlog/msty-ai-offline-support.md for planned offline support
# Ollama support removed in favor of future Msty.ai integration

# -----------------------------------------------------------------------------
# QUICK START FOR LOCAL DEVELOPMENT
# -----------------------------------------------------------------------------
# For a minimal local setup, you can leave all values empty:
#
# 1. Copy this file:
#    cp .env.example .env.local
#
# 2. Start the dev server:
#    npm run dev
#
# The app will work with the following graceful degradation:
# ✅ Contact form shows but logs submissions (no email sent)
# ✅ GitHub heatmap works with lower rate limits (no token)
# ✅ Blog posts work without view counts (no Redis)
# ✅ All pages and features load successfully
#
# -----------------------------------------------------------------------------
# PRODUCTION SETUP
# -----------------------------------------------------------------------------
# For a production deployment, configure at minimum:
#
# REQUIRED:
# - RESEND_API_KEY (for contact form emails)
#
# RECOMMENDED:
# - SENTRY_DSN + NEXT_PUBLIC_SENTRY_DSN (for error tracking)
# - SENTRY_AUTH_TOKEN (for source map uploads in CI/CD)
# - GITHUB_TOKEN (for better rate limits on heatmap)
# - REDIS_URL (for view count tracking)
#
# OPTIONAL:
# - NEXT_PUBLIC_SITE_URL or NEXT_PUBLIC_SITE_DOMAIN (if custom domain)
#
# On Vercel, set these in: Project Settings → Environment Variables
# https://vercel.com/docs/projects/environment-variables